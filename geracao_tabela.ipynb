{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c2270a",
   "metadata": {},
   "source": [
    "# Research questions datasheet generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ad3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e325676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_list(string):\n",
    "    match = re.findall(\"'(.*?)'\", string)\n",
    "    return match\n",
    "\n",
    "def extract_mean(value):\n",
    "    if value is not None and isinstance(value, str):\n",
    "        lines = value.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('mean'):\n",
    "                return float(line.split()[1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8958cc2",
   "metadata": {},
   "source": [
    "## RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1303bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "\n",
    "new_df = pd.DataFrame(columns=[\"System\",\"Algorithm\",\"Feature Selection\",\"config_size\",\"N=30\",\"N=50\",\"N=70\",\"N=90\"])\n",
    "\n",
    "system_sizes = {}\n",
    "\n",
    "for system in systems:\n",
    "    path = f'results/{system}/{system}-results_random_t3_full_v2.csv'\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    combined_df = df.copy()\n",
    "\n",
    "    for algorithm in combined_df['algorithm_name'].unique():\n",
    "        for sampling in combined_df['origin_ft_selection'].unique():\n",
    "\n",
    "            temp_df = combined_df[(combined_df['algorithm_name'] == algorithm) & \n",
    "                                (combined_df['origin_ft_selection'] == sampling)]\n",
    "\n",
    "            accuracy_30 = temp_df[temp_df['test_size'] == 0.3]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.3]) > 0 else None\n",
    "            accuracy_50 = temp_df[temp_df['test_size'] == 0.5]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.5]) > 0 else None\n",
    "            accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "            accuracy_90 = temp_df[temp_df['test_size'] == 0.9]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.9]) > 0 else None\n",
    "\n",
    "            new_df = new_df.append({\"System\": system,\n",
    "                                    \"Algorithm\": algorithm, \n",
    "                                    \"Feature Selection\": sampling,\n",
    "                                    \"config_size\": 0, \n",
    "                                    \"N=30\": accuracy_30,\n",
    "                                    \"N=50\": accuracy_50, \n",
    "                                    \"N=70\": accuracy_70, \n",
    "                                    \"N=90\": accuracy_90}, ignore_index=True)\n",
    "\n",
    "for system in systems:\n",
    "    path = system+\"/sampledConfigurations_random_t3.csv\"\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    config_size = len(df)\n",
    "\n",
    "    new_df.loc[new_df['System'] == system, 'config_size'] = config_size\n",
    "\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].apply(extract_mean).astype(float)\n",
    "new_df['N=50'] = new_df['N=50'].apply(extract_mean).astype(float)\n",
    "new_df['N=70'] = new_df['N=70'].apply(extract_mean).astype(float)\n",
    "new_df['N=90'] = new_df['N=90'].apply(extract_mean).astype(float)\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=50'] = new_df['N=50'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=70'] = new_df['N=70'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=90'] = new_df['N=90'].apply(lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].astype(float)\n",
    "new_df['N=50'] = new_df['N=50'].astype(float)\n",
    "new_df['N=70'] = new_df['N=70'].astype(float)\n",
    "new_df['N=90'] = new_df['N=90'].astype(float)\n",
    "\n",
    "\n",
    "new_df['Weighted Mean MAPE'] = new_df.apply(lambda row: np.average([row['N=30'], row['N=50'], row['N=70'], row['N=90']], \n",
    "                                                                   weights=[0.3*row['config_size'], 0.5*row['config_size'], 0.7*row['config_size'], 0.9*row['config_size']]), axis=1)\n",
    "\n",
    "best_algorithm = new_df.loc[new_df['Weighted Mean MAPE'].idxmin()]['Algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d89738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>N=30</th>\n",
       "      <th>N=50</th>\n",
       "      <th>N=70</th>\n",
       "      <th>N=90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7z</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BerkeleyDBC</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dune</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>6.73</td>\n",
       "      <td>5.32</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hipacc</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Irzip</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>12.73</td>\n",
       "      <td>13.39</td>\n",
       "      <td>11.07</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLVM</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polly</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x264</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        System         Algorithm   N=30   N=50   N=70  N=90\n",
       "0           7z      RandomForest   4.53   3.73   3.45  3.35\n",
       "1  BerkeleyDBC      DecisionTree   5.98   1.01   0.45  0.58\n",
       "2         Dune      RandomForest   6.73   5.32   4.94  4.61\n",
       "3       Hipacc      RandomForest   3.59   2.18   1.66  1.54\n",
       "4        Irzip      DecisionTree  12.73  13.39  11.07  2.79\n",
       "5         LLVM  LinearRegression   2.65   2.88   3.18  2.88\n",
       "6        Polly      RandomForest   1.70   1.24   1.17  1.06\n",
       "7         x264      DecisionTree   2.13   1.40   0.57  0.18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lllrrrr}\\n\\\\toprule\\n{} &       System &         Algorithm &   N=30 &   N=50 &   N=70 &  N=90 \\\\\\\\\\n\\\\midrule\\n0 &           7z &      RandomForest &   4.53 &   3.73 &   3.45 &  3.35 \\\\\\\\\\n1 &  BerkeleyDBC &      DecisionTree &   5.98 &   1.01 &   0.45 &  0.58 \\\\\\\\\\n2 &         Dune &      RandomForest &   6.73 &   5.32 &   4.94 &  4.61 \\\\\\\\\\n3 &       Hipacc &      RandomForest &   3.59 &   2.18 &   1.66 &  1.54 \\\\\\\\\\n4 &        Irzip &      DecisionTree &  12.73 &  13.39 &  11.07 &  2.79 \\\\\\\\\\n5 &         LLVM &  LinearRegression &   2.65 &   2.88 &   3.18 &  2.88 \\\\\\\\\\n6 &        Polly &      RandomForest &   1.70 &   1.24 &   1.17 &  1.06 \\\\\\\\\\n7 &         x264 &      DecisionTree &   2.13 &   1.40 &   0.57 &  0.18 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_copy = new_df.copy()\n",
    "\n",
    "new_df_copy = new_df_copy.sort_values('System')\n",
    "\n",
    "idx = new_df_copy.groupby('System')['Weighted Mean MAPE'].idxmin()\n",
    "\n",
    "best_algorithms = new_df_copy.loc[idx]\n",
    "\n",
    "best_algorithms.reset_index(drop=True, inplace=True)\n",
    "\n",
    "best_algorithms.drop(columns=['Weighted Mean MAPE', 'Feature Selection', 'config_size'], inplace=True)\n",
    "\n",
    "display(best_algorithms)\n",
    "\n",
    "best_algorithms.to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ad0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "def find_maior_numero(path):\n",
    "    arquivos = glob.glob(path)\n",
    "\n",
    "    maior_numero = -1\n",
    "    arquivo_maior_numero = None\n",
    "\n",
    "    for arquivo in arquivos:\n",
    "        match = re.search(r\"engine_(\\d+)_v3\", arquivo)\n",
    "        if match:\n",
    "            numero = int(match.group(1))\n",
    "            if numero > maior_numero:\n",
    "                maior_numero = numero\n",
    "                arquivo_maior_numero = arquivo\n",
    "    \n",
    "    return arquivo_maior_numero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66585e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['num_features'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8705016324e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0marquivo_maior_numero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_maior_numero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf_novo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marquivo_maior_numero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_novo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_novo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'origin_ft_selection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf_novo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_novo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['num_features'] not in index\""
     ]
    }
   ],
   "source": [
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "df_best_alg_fs = pd.DataFrame(columns=[\"System\",\"Algorithm\",\"Feature Selection\",'config_size', \"num_features\", \"N=30\",\"N=50\",\"N=70\",\"N=90\"])\n",
    "\n",
    "for system in systems:\n",
    "    \n",
    "    path = f'results/{system}/{system}-results_with_feature_importance_random_t3_features_engine_[0-9]*.csv'\n",
    "    arquivo_maior_numero = find_maior_numero(path)\n",
    "    df_novo = pd.read_csv(arquivo_maior_numero)\n",
    "    df_novo = df_novo[['algorithm_name', 'origin_ft_selection', 'test_size', 'accuracy', 'num_features']]\n",
    "    df_novo['accuracy'] = df_novo['accuracy'].apply(extract_mean).astype(float)    \n",
    "\n",
    "    for algorithm in df_novo['algorithm_name'].unique():\n",
    "        for sampling in df_novo['origin_ft_selection'].unique():\n",
    "            for features in df_novo['num_features'].unique():\n",
    "                temp_df = df_novo[(df_novo['algorithm_name'] == algorithm) & \n",
    "                                    (df_novo['origin_ft_selection'] == sampling) & \n",
    "                                     (df_novo['num_features'] == features)]\n",
    "\n",
    "                accuracy_30 = temp_df[temp_df['test_size'] == 0.3]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.3]) > 0 else None\n",
    "                accuracy_50 = temp_df[temp_df['test_size'] == 0.5]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.5]) > 0 else None\n",
    "                accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "                accuracy_90 = temp_df[temp_df['test_size'] == 0.9]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.9]) > 0 else None\n",
    "\n",
    "                df_best_alg_fs = df_best_alg_fs.append({\"System\": system,\n",
    "                                                        \"Algorithm\": algorithm, \n",
    "                                                        \"Feature Selection\": sampling,\n",
    "                                                        \"config_size\": 0,\n",
    "                                                        \"num_features\": features,\n",
    "                                                        \"N=30\": accuracy_30,\n",
    "                                                        \"N=50\": accuracy_50, \n",
    "                                                        \"N=70\": accuracy_70, \n",
    "                                                        \"N=90\": accuracy_90}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_alg_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36916170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_percentages(group):\n",
    "    unique_values = group.unique()\n",
    "    value_to_percentage = {value: percentage for value, percentage in zip(sorted(unique_values), [30, 40, 50, 60, 100])}\n",
    "    return group.map(value_to_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9325f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_best_alg_fs_temp = df_best_alg_fs[(df_best_alg_fs['System'] == '7z') & (df_best_alg_fs['Algorithm'] == 'RandomForest') | \n",
    "#               (df_best_alg_fs['System'] == 'BerkeleyDBC') & (df_best_alg_fs['Algorithm'] == \"DecisionTree\") | \n",
    "#             (df_best_alg_fs['System'] == 'Dune') & (df_best_alg_fs['Algorithm'] == \"RandomForest\") |\n",
    "#             (df_best_alg_fs['System'] == 'Hipacc') & (df_best_alg_fs['Algorithm'] == \"RandomForest\") |\n",
    "#             (df_best_alg_fs['System'] == 'Irzip') & (df_best_alg_fs['Algorithm'] == \"DecisionTree\") |\n",
    "#             (df_best_alg_fs['System'] == 'LLVM') & (df_best_alg_fs['Algorithm'] == \"LinearRegression\") |\n",
    "#             (df_best_alg_fs['System'] == 'Polly') & (df_best_alg_fs['Algorithm'] == \"RandomForest\") | \n",
    "#             (df_best_alg_fs['System'] == 'x264') & (df_best_alg_fs['Algorithm'] == \"DecisionTree\")]\n",
    "\n",
    "df_temp_all = df_best_alg_fs.copy()\n",
    "df_rq1 = df_best_alg_fs[['System', 'Algorithm', 'config_size', 'N=30', 'N=50', 'N=70', 'N=90']]\n",
    "\n",
    "# df_temp_all.sort_values(by=['System', 'Algorithm', 'num_features'], inplace=True)\n",
    "df_temp_all['Percentage'] = df_temp_all.groupby(['System', 'Algorithm'])['num_features'].transform(map_percentages)\n",
    "df_temp_all = df_temp_all[df_temp_all['Percentage'] == 50]\n",
    "\n",
    "# df_temp_all = df_temp_all[df_temp_all['System'] != 'Irzip']\n",
    "df_temp_all.drop(columns=['Feature Selection', 'num_features'])\n",
    "\n",
    "for system in systems:\n",
    "    path = system+\"/sampledConfigurations_random_t3.csv\"\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    config_size = len(df)\n",
    "\n",
    "    df_temp_all.loc[df_temp_all['System'] == system, 'config_size'] = config_size\n",
    "    \n",
    "df_temp_all['Weighted Mean MAPE'] = df_temp_all.apply(lambda row: np.average([row['N=30'], row['N=50'], row['N=70'], row['N=90']], \n",
    "                                                                   weights=[0.3*row['config_size'], 0.5*row['config_size'], 0.7*row['config_size'], 0.9*row['config_size']]), axis=1)\n",
    "\n",
    "df_temp_all_copy = df_temp_all.copy()\n",
    "\n",
    "df_temp_all_copy = df_temp_all_copy.sort_values('System')\n",
    "\n",
    "idx = df_temp_all_copy.groupby('System')['Weighted Mean MAPE'].idxmin()\n",
    "\n",
    "best_algorithms_fs = df_temp_all_copy.loc[idx]\n",
    "\n",
    "best_algorithms_fs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "best_algorithms_fs.drop(columns=['Weighted Mean MAPE', 'Feature Selection', 'config_size', 'Percentage', 'num_features'], inplace=True)\n",
    "\n",
    "display(best_algorithms_fs)\n",
    "\n",
    "best_algorithms_fs.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_lrzip = df_best_alg_fs[['System', 'Algorithm', 'Feature Selection', 'num_features','N=90']]\n",
    "\n",
    "df_temp_lrzip.sort_values(by=['System', 'Algorithm', 'num_features'], inplace=True)\n",
    "df_rq1.sort_values(by=['System', 'Algorithm', 'num_features'], inplace=True)\n",
    "\n",
    "df_temp_lrzip = df_temp_lrzip.rename(columns={'N=90': 'N=70'})\n",
    "\n",
    "df_temp_lrzip = df_temp_lrzip[(df_temp_lrzip['System'] == 'Irzip') & (df_temp_lrzip['Algorithm'] == 'DecisionTree')]\n",
    "\n",
    "df_temp_lrzip['Percentage'] = df_temp_lrzip.groupby(['System', 'Algorithm'])['num_features'].transform(map_percentages)\n",
    "df_rq1['Percentage'] = df_rq1.groupby(['System', 'Algorithm'])['num_features'].transform(map_percentages)\n",
    "df_temp_lrzip = df_temp_lrzip.drop(columns=['Feature Selection', 'num_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_temp_all, df_temp_lrzip], ignore_index=True)\n",
    "# df_all = df_all[df_all['Percentage'] == 50]\n",
    "df_all = df_all.drop(columns=['Feature Selection', 'num_features'])\n",
    "df_all\n",
    "\n",
    "for system in systems:\n",
    "    display(system)\n",
    "    display(df_all[df_all['System']==system]['config_size'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "grouped = df_all.groupby(['System', 'Algorithm'])\n",
    "\n",
    "for name, group in grouped:\n",
    "    if name[0] == 'Irzip':\n",
    "        plt.plot(group['Percentage'], group['N=70'], marker='o', linestyle='-', label=f'{name} N=90')\n",
    "    else:\n",
    "        plt.plot(group['Percentage'], group['N=70'], marker='o', linestyle='-', label=f'{name} N=70')\n",
    "\n",
    "plt.xlabel('Feature Percentage (%)')\n",
    "plt.ylabel('MAPE')\n",
    "plt.title('MAPE per Feature Percentage for each System and Algorithm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq1 \n",
    "\n",
    "# df_rq1 = df_rq1[df_rq1['Percentage'] == 50]\n",
    "# df_rq1.drop(columns=['Percentage', 'num_features'], inplace=True)\n",
    "# df_rq1.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "\n",
    "new_df = pd.DataFrame(columns=[\"System\",\"Algorithm\",\"Feature Selection\",\"config_size\",\"N=30\",\"N=50\",\"N=70\",\"N=90\"])\n",
    "\n",
    "system_sizes = {}\n",
    "\n",
    "for system in systems:\n",
    "    path = f'results/{systems[2]}/{systems[2]}-results_with_feature_importance_random_t3_features_engine_[0-9]*.csv'\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    combined_df = df.copy()\n",
    "\n",
    "    for algorithm in combined_df['algorithm_name'].unique():\n",
    "        for sampling in combined_df['origin_ft_selection'].unique():\n",
    "\n",
    "            temp_df = combined_df[(combined_df['algorithm_name'] == algorithm) & \n",
    "                                (combined_df['origin_ft_selection'] == sampling)]\n",
    "\n",
    "            accuracy_30 = temp_df[temp_df['test_size'] == 0.3]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.3]) > 0 else None\n",
    "            accuracy_50 = temp_df[temp_df['test_size'] == 0.5]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.5]) > 0 else None\n",
    "            accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "            accuracy_90 = temp_df[temp_df['test_size'] == 0.9]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.9]) > 0 else None\n",
    "\n",
    "            new_df = new_df.append({\"System\": system,\n",
    "                                    \"Algorithm\": algorithm, \n",
    "                                    \"Feature Selection\": sampling,\n",
    "                                    \"config_size\": 0, \n",
    "                                    \"N=30\": accuracy_30,\n",
    "                                    \"N=50\": accuracy_50, \n",
    "                                    \"N=70\": accuracy_70, \n",
    "                                    \"N=90\": accuracy_90}, ignore_index=True)\n",
    "\n",
    "for system in systems:\n",
    "    path = system+\"/sampledConfigurations_random_t3.csv\"\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    config_size = len(df)\n",
    "\n",
    "    new_df.loc[new_df['System'] == system, 'config_size'] = config_size\n",
    "\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].apply(extract_mean).astype(float)\n",
    "new_df['N=50'] = new_df['N=50'].apply(extract_mean).astype(float)\n",
    "new_df['N=70'] = new_df['N=70'].apply(extract_mean).astype(float)\n",
    "new_df['N=90'] = new_df['N=90'].apply(extract_mean).astype(float)\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=50'] = new_df['N=50'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=70'] = new_df['N=70'].apply(lambda x: '{:.2f}'.format(x))\n",
    "new_df['N=90'] = new_df['N=90'].apply(lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "new_df['N=30'] = new_df['N=30'].astype(float)\n",
    "new_df['N=50'] = new_df['N=50'].astype(float)\n",
    "new_df['N=70'] = new_df['N=70'].astype(float)\n",
    "new_df['N=90'] = new_df['N=90'].astype(float)\n",
    "\n",
    "\n",
    "new_df['Weighted Mean MAPE'] = new_df.apply(lambda row: np.average([row['N=30'], row['N=50'], row['N=70'], row['N=90']], \n",
    "                                                                   weights=[0.3*row['config_size'], 0.5*row['config_size'], 0.7*row['config_size'], 0.9*row['config_size']]), axis=1)\n",
    "\n",
    "best_algorithm = new_df.loc[new_df['Weighted Mean MAPE'].idxmin()]['Algorithm']\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicar para número de features\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "systems = ['x264']\n",
    "\n",
    "df_melt = df_best_alg_fs.melt(id_vars=['System', 'Algorithm', 'Feature Selection', 'config_size', 'Weighted Mean MAPE'],\n",
    "                  value_vars=['N=30', 'N=50', 'N=70', 'N=90'],\n",
    "                  var_name='Feature Number',\n",
    "                  value_name='MAPE')\n",
    "\n",
    "df_melt['Feature Number'] = df_melt['Feature Number'].str.replace('N=', '').astype(int)\n",
    "\n",
    "grouped = df_melt.groupby(['System', 'Feature Number']).mean().reset_index()\n",
    "\n",
    "for system in grouped['System'].unique():\n",
    "    subset = grouped[grouped['System'] == system]\n",
    "    plt.plot(subset['Feature Number'], subset['MAPE'], marker='o', label=system)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('MAPE')\n",
    "plt.title('MAPE for Different Feature Numbers and Systems')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ec471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "samples_config = ['random']\n",
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "samples_config = ['distBased', 'divDistBased','henard', 'random', 'solverBased', 'twise']\n",
    "# Number of 60% of features in order of above systems list: 'Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z' \n",
    "max_features = [17,7,9,11,12,20,31,25]\n",
    "\n",
    "system_name = 'Dune'\n",
    "num_features = '17'\n",
    "ft = False\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "if ft:\n",
    "    file_pattern = \"results/\"+system_name+\"/\"+system_name+\"-results_with_feature_importance_random_t3_features_engine_\"+num_features+\"_v2.csv\"\n",
    "else:\n",
    "    file_pattern = \"results/\"+system_name+\"/\"+system_name+\"-results_random_t3_full_v2.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_pattern, sep=',')\n",
    "display(df)\n",
    "df['system'] = system_name\n",
    "df['ft_selection_technique'] = 'random'\n",
    "combined_df = df.copy()\n",
    "\n",
    "#for sample in samples_config:\n",
    "    #file_pattern = f'results/{systems[2]}/{systems[2]}-results_with_feature_importance_{sample}_t3_features_engine_[0-9]*.csv'\n",
    "\n",
    "    #file_pattern = f'results/{systems[0]}/{systems[0]}-results_with_feature_importance_{sample}_t3_features_engine_11.csv'\n",
    "\n",
    "#     file_pattern = f'results/bkp-with_dummy/{system}/{system}-results_with_feature_importance_{sample}_t3_features_engine_*.csv'\n",
    "#     file_pattern = f'results/{system}/{system}-results_with_feature_importance_{sample}_t3_features_engine_*.csv'\n",
    "\n",
    "#    file_list = glob.glob(file_pattern)\n",
    "#    for file in file_list:\n",
    "#        df = pd.read_csv(file, sep=',')\n",
    "#        print(file)\n",
    "#        display(df)\n",
    "#        number = file.rsplit('_', 1)[-1].split('.')[0]\n",
    "#        #print(len(combined_df))\n",
    "#        combined_df = pd.concat([combined_df, df])\n",
    "        \n",
    "\n",
    "\n",
    "#combined_df.reset_index(drop=True, inplace=True)\n",
    "#combined_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80609876",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec00cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "def _getDataBase(system, sample, ft=False, num_features=0):\n",
    "    combined_df = pd.DataFrame()\n",
    "    if ft:\n",
    "        file_pattern = f\"results/{system}/{system}-results_with_feature_importance_{sample}_t3_features_engine_{num_features}_v3.csv\"\n",
    "    else:\n",
    "        file_pattern = f\"results/{system}/{system}-results_{sample}_t3_full_v2.csv\"\n",
    "    \n",
    "    df = pd.read_csv(file_pattern, sep=',')\n",
    "    df['technique'] = technique\n",
    "    df['system'] = system\n",
    "    df['ft_selection_technique'] = sample\n",
    "    combined_df = df.copy()\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getNumFeatures(system):\n",
    "    if system == '7z': \n",
    "        num_features = [12,16,20,25]\n",
    "    elif system == 'BerkeleyDBC':\n",
    "        num_features = [5,7,9,11]\n",
    "    elif system == 'Dune':\n",
    "        num_features = [8,11,14,17]\n",
    "    elif system == 'Hipacc':\n",
    "        num_features = [16,21,26,31]\n",
    "    elif system == 'Irzip':\n",
    "        num_features = [6,8,10,12]\n",
    "    elif system == 'LLVM':\n",
    "        num_features = [4,5,6,7]\n",
    "    elif system == 'Polly':\n",
    "        num_features = [10,13,16,20]\n",
    "    elif system == 'x264':\n",
    "        num_features = [4,6,8,9]\n",
    "        \n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_getNumFeatures('7z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad7016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "samples_config = ['distBased', 'divDistBased','henard', 'random', 'solverBased', 'twise']\n",
    "\n",
    "new_df_2 = pd.DataFrame(columns=[\"System\",\"Algorithm\",\"Feature Selection\",\"Technique\",\"N=30\",\"N=50\",\"N=70\",\"N=90\"])\n",
    "\n",
    "for system in systems:\n",
    "    num_features = _getNumFeatures(system)\n",
    "    new_df = pd.DataFrame(columns=['algorithm_name','ft_dummyfication','origin_ft_selection','ft_selection','hyperparameters','size_target','test_size','coef_order','accuracy','num_features'])\n",
    "    \n",
    "    for sample in samples_config:\n",
    "        for ft in [True, False]:\n",
    "            if ft:\n",
    "                path = f'results/{system}/{system}-results_with_feature_importance_{sample}_t3_features_engine_[0-9]*.csv'\n",
    "                arquivo_maior_numero = find_maior_numero(path)\n",
    "                df = pd.read_csv(arquivo_maior_numero)\n",
    "                df['sample'] = sample\n",
    "                df['system'] = system\n",
    "                new_df = pd.concat([new_df, df])\n",
    "            else:\n",
    "                path = f\"results/{system}/{system}-results_{sample}_t3_full_v2.csv\"\n",
    "                df = pd.read_csv(path)\n",
    "                df_columns = pd.read_csv(f\"{system}/sampledConfigurations_solverBased_t3.csv\", sep=\";\")\n",
    "                df['num_features'] = len(df_columns.columns) - 1\n",
    "                df['sample'] = sample\n",
    "                df['system'] = system\n",
    "                new_df = pd.concat([new_df, df])\n",
    "          \n",
    "                        \n",
    "    csv_path = 'results/'+system+'/'+system+'_summary_results_2.csv'\n",
    "    new_df = new_df[['system', 'algorithm_name', 'test_size', 'num_features', 'accuracy', 'sample']]\n",
    "    new_df = new_df.drop_duplicates(subset=['system', 'algorithm_name', 'test_size', 'num_features', 'accuracy']).reset_index(drop=True)\n",
    "    new_df['accuracy'] = new_df['accuracy'].apply(extract_mean).astype(float).round(2)\n",
    "    new_df.to_csv(csv_path)\n",
    "    \n",
    "\n",
    "# new_df.drop(columns=['ft_dummyfication', 'Unnamed: 0.1', 'ft_selection', 'hyperparameters', 'size_target', 'coef_order', 'Unnamed: 0'])\n",
    "# new_df['Percentage'] = df_temp_all.groupby(['algorithm_name', 'Algorithm', 'Technique'])['num_features'].transform(map_percentages)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6473a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(columns=new_df.columns)\n",
    "\n",
    "new_df_all = pd.DataFrame(columns=[\"System\",\"Algorithm\",\"Sample\",\"N=70\", \"Percentage\"])\n",
    "\n",
    "for system in systems:\n",
    "    csv_path = 'results/'+system+'/'+system+'_summary_results_2.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_all = pd.concat([df_all, df],ignore_index=True)\n",
    "    \n",
    "    display(df_all)\n",
    "\n",
    "df_all['Percentage'] = df_all.groupby(['algorithm_name', 'system', 'sample'])['num_features'].transform(map_percentages)\n",
    "df_all = df_all[((df_all['Percentage'] == 50) | (df_all['Percentage'] == 100)) & (df_all['test_size'] == 0.7)]\n",
    "df_all = df_all.drop(columns=['num_features']).reset_index(drop=True)\n",
    "\n",
    "display(df_all) \n",
    "for algorithm in df_all['algorithm_name'].unique():\n",
    "    for sample in df_all['sample'].unique():\n",
    "        for system in df_all['system'].unique(): \n",
    "            for pct in df_all['Percentage'].unique():\n",
    "                temp_df = df_all[(df_all['algorithm_name'] == algorithm) & \n",
    "                                                    (df_all['sample'] == sample) & \n",
    "                                                    (df_all['system'] == system) &\n",
    "                                                    (df_all['Percentage'] == pct)]\n",
    "\n",
    "                accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "\n",
    "                new_df_all = new_df_all.append({\"System\": system,\n",
    "                                        \"Algorithm\": algorithm, \n",
    "                                        \"Sample\": sample, \n",
    "                                        \"N=70\": accuracy_70,\n",
    "                                        \"Percentage\": pct}, ignore_index=True)\n",
    "                    \n",
    "new_df_all['N=70'] = new_df_all['N=70'].astype(float)\n",
    "new_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a5b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a função pivot_table() para montar a tabela desejada\n",
    "df_pivot = pd.pivot_table(new_df_all, values='N=70', index=['System', 'Algorithm'], columns=['Sample', 'Percentage'])\n",
    "\n",
    "# Substituir valores NaN por '-'\n",
    "df_pivot = df_pivot.fillna('-')\n",
    "\n",
    "# Converter valores em notação científica para float com 2 casas decimais\n",
    "df_pivot = df_pivot.applymap(lambda x: '{:.2f}'.format(x) if isinstance(x, float) else x)\n",
    "\n",
    "# Substituir valores maiores que 1000 por '-'\n",
    "df_pivot = df_pivot.applymap(lambda x: '-' if isinstance(x, float) and x > 1000 else x)\n",
    "\n",
    "df_pivot.to_latex(multirow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2338dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(\"results/LLVM/LLVM_summary_results_2.csv\")\n",
    "df_temp[df_temp['test_size'] == 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if ft:\n",
    "                for algorithm in combined_df['algorithm_name'].unique():\n",
    "                    for sampling in combined_df['origin_ft_selection'].unique():\n",
    "                        for system in combined_df['system'].unique(): \n",
    "                            for features in combined_df['num_features'].unique():\n",
    "                                for technique in combined_df['ft_selection_technique'].unique():\n",
    "\n",
    "                                    temp_df = combined_df[(combined_df['algorithm_name'] == algorithm) & \n",
    "                                                        (combined_df['origin_ft_selection'] == sampling) & \n",
    "                                                        (combined_df['system'] == system) &\n",
    "                                                          (combined_df['num_features'] == features) &\n",
    "                                                        (combined_df['ft_selection_technique'] == technique)]\n",
    "\n",
    "                                    accuracy_30 = temp_df[temp_df['test_size'] == 0.3]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.3]) > 0 else None\n",
    "                                    accuracy_50 = temp_df[temp_df['test_size'] == 0.5]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.5]) > 0 else None\n",
    "                                    accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "                                    accuracy_90 = temp_df[temp_df['test_size'] == 0.9]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.9]) > 0 else None\n",
    "\n",
    "                                    new_df = new_df.append({\"System\": system,\n",
    "                                                            \"Algorithm\": algorithm, \n",
    "                                                            \"Feature Selection\": sampling, \n",
    "                                                            \"Technique\": technique,\n",
    "                                                            \"num_features\": features,\n",
    "                                                            \"N=30\": accuracy_30,\n",
    "                                                            \"N=50\": accuracy_50, \n",
    "                                                            \"N=70\": accuracy_70, \n",
    "                                                            \"N=90\": accuracy_90}, ignore_index=True)\n",
    "            else:\n",
    "                for algorithm in combined_df['algorithm_name'].unique():\n",
    "                    for sampling in combined_df['origin_ft_selection'].unique():\n",
    "                        for system in combined_df['system'].unique(): \n",
    "                            for technique in combined_df['ft_selection_technique'].unique():\n",
    "                                temp_df = combined_df[(combined_df['algorithm_name'] == algorithm) & \n",
    "                                                    (combined_df['origin_ft_selection'] == sampling) & \n",
    "                                                    (combined_df['system'] == system) &\n",
    "                                                    (combined_df['ft_selection_technique'] == technique)]\n",
    "\n",
    "                                accuracy_30 = temp_df[temp_df['test_size'] == 0.3]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.3]) > 0 else None\n",
    "                                accuracy_50 = temp_df[temp_df['test_size'] == 0.5]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.5]) > 0 else None\n",
    "                                accuracy_70 = temp_df[temp_df['test_size'] == 0.7]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.7]) > 0 else None\n",
    "                                accuracy_90 = temp_df[temp_df['test_size'] == 0.9]['accuracy'].values[0] if len(temp_df[temp_df['test_size'] == 0.9]) > 0 else None\n",
    "\n",
    "                                new_df = new_df.append({\"System\": system,\n",
    "                                                        \"Algorithm\": algorithm, \n",
    "                                                        \"Feature Selection\": sampling, \n",
    "                                                        \"Technique\": technique,\n",
    "                                                        \"num_features\": len(df),\n",
    "                                                        \"N=30\": accuracy_30,\n",
    "                                                        \"N=50\": accuracy_50, \n",
    "                                                        \"N=70\": accuracy_70, \n",
    "                                                        \"N=90\": accuracy_90}, ignore_index=True)\n",
    "\n",
    "                                \n",
    "\n",
    "                new_df['N=30'] = new_df['N=30'].apply(extract_mean)\n",
    "                new_df['N=50'] = new_df['N=50'].apply(extract_mean)\n",
    "                new_df['N=70'] = new_df['N=70'].apply(extract_mean)\n",
    "                new_df['N=90'] = new_df['N=90'].apply(extract_mean)\n",
    "\n",
    "                new_df['N=30'] = new_df['N=30'].apply(lambda x: '{:.2f}'.format(x))\n",
    "                new_df['N=50'] = new_df['N=50'].apply(lambda x: '{:.2f}'.format(x))\n",
    "                new_df['N=70'] = new_df['N=70'].apply(lambda x: '{:.2f}'.format(x))\n",
    "                new_df['N=90'] = new_df['N=90'].apply(lambda x: '{:.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from anytree import Node, RenderTree\n",
    "\n",
    "def generate_feature_model_tree(root_element):\n",
    "    print()\n",
    "    root = Node(root_element.attrib[\"name\"])\n",
    "    for child_element in root_element:\n",
    "        child_node = generate_feature_model_tree(child_element)\n",
    "        child_node.parent = root\n",
    "    return root\n",
    "\n",
    "def generate_feature_model_diagram(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root_element = tree.getroot()\n",
    "    root = generate_feature_model_tree(root_element)\n",
    "    for pre, fill, node in RenderTree(root):\n",
    "        print(f\"{pre}{node.name}\")\n",
    "\n",
    "# Exemplo de uso:\n",
    "file_path = \"FeatureModel.xml\"\n",
    "generate_feature_model_diagram(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4216216",
   "metadata": {},
   "source": [
    "# RQ4: Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# system = 'LLVM'\n",
    "systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','7z', 'Hipacc']\n",
    "samples_config = ['distBased', 'divDistBased','henard', 'random', 'solverBased', 'twise']\n",
    "\n",
    "df_time_summary = pd.DataFrame(columns=[\"System\",\"Sample Algorithm\",\"Feature Selection\",\"Algorithm\", \"%Features\", \"N=10%\", \"N=20%\", \"N=50%\", \"N=80%\", \"N=90%\"])\n",
    "df_time_per_sample = pd.DataFrame(columns=[\"System\",\"Sample Algorithm\",\"Feature Selection\",\"Total Elapsed Time\"])\n",
    "\n",
    "df_time_summary_all = df_time_summary\n",
    "df_time_per_sample_all = df_time_per_sample\n",
    "df_time_all_systems = pd.DataFrame(columns=[\"System\", \"Total Elapsed Time\", \"Total Elapsed Time with Feature Selection\"])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for system in systems:\n",
    "    training_time_file = r'results/'+system+'/'+system+'-simulation_time_information.csv'\n",
    "    training_time_file_v2 = r'results/'+system+'/'+system+'-simulation_time_information_v2.csv'\n",
    "    df_time = pd.read_csv(training_time_file, sep=',')\n",
    "    df_time = pd.concat([df_time, pd.read_csv(training_time_file_v2, sep=',')], ignore_index=True)\n",
    "    \n",
    "    df_time.append(pd.read_csv(training_time_file_v2, sep=','))\n",
    "    \n",
    "    training_time_file_ft = r'results/'+system+'/'+system+'-simulation_time_information-with_feature_importance.csv'\n",
    "    training_time_file_ft_v2 = r'results/'+system+'/'+system+'-simulation_time_information-with_feature_importance_v2.csv'\n",
    "    df_time_ft = pd.read_csv(training_time_file_ft, sep=',')\n",
    "    df_time_ft = pd.concat([df_time_ft, pd.read_csv(training_time_file_ft_v2, sep=',')], ignore_index=True)\n",
    "\n",
    "    df_time[\"StartTime\"] = pd.to_datetime(df_time[\"StartTime\"])\n",
    "    df_time[\"EndTime\"] = pd.to_datetime(df_time[\"EndTime\"])\n",
    "    df_time[\"ElapsedTime\"] = (df_time[\"EndTime\"] - df_time[\"StartTime\"]).dt.seconds\n",
    "    df_time[\"FT\"] = False\n",
    "    df_time[\"System\"] = system\n",
    "    \n",
    "#     display(df_time)\n",
    "    \n",
    "#     display(df_time[0])\n",
    "    \n",
    "    df_time_ft[\"StartTime\"] = pd.to_datetime(df_time_ft[\"StartTime\"])\n",
    "    df_time_ft[\"EndTime\"] = pd.to_datetime(df_time_ft[\"EndTime\"])\n",
    "    df_time_ft[\"ElapsedTime\"] = (df_time_ft[\"EndTime\"] - df_time_ft[\"StartTime\"]).dt.seconds\n",
    "    df_time_ft[\"FT\"] = True\n",
    "    df_time_ft[\"System\"] = str(system)\n",
    "    \n",
    "    \n",
    "#     df_time_combined = [(df_time, df_time_ft)]\n",
    "    df_time_combined = pd.concat([df_time, df_time_ft], ignore_index=True)\n",
    "    \n",
    "    \n",
    "#     display(df_time_combined)\n",
    "\n",
    "#     print(df_time_combined[(df_time_combined[\"%Configurations\"] == 0.1) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean())\n",
    "\n",
    "    data_list = []\n",
    "    data_list_sample = []\n",
    "    data_list_all = []\n",
    "    \n",
    "    data_dict3 = {\n",
    "                \"System\": system,\n",
    "                \"Total Elapsed Time\": df_time_combined[(df_time_combined[\"FT\"] == False) & (df_time_combined[\"System\"] == system)][\"ElapsedTime\"].sum(),\n",
    "                \"Total Elapsed Time with Feature Selection\": df_time_combined[(df_time_combined[\"FT\"] == True) & (df_time_combined[\"System\"] == system)][\"ElapsedTime\"].sum()\n",
    "            }\n",
    "    data_list_all.append(data_dict3)\n",
    "\n",
    "    for sample in samples_config:\n",
    "        \n",
    "        for algorithm in df_time[\"Algorithm\"].unique():\n",
    "            for ft in [True, False]:\n",
    "                data_dict2 = {\n",
    "                        \"System\": system,\n",
    "                        \"Sample Algorithm\": sample,\n",
    "                        \"Feature Selection\": ft,\n",
    "                        \"Total Elapsed Time\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"System\"] == system) & (df_time_combined[\"SampleAlgorithm\"] == sample) & (df_time_combined[\"%Configurations\"].isna().any())][\"ElapsedTime\"].sum()\n",
    "                    }\n",
    "                data_list_sample.append(data_dict2)\n",
    "            \n",
    "                \n",
    "                for features_number in [0.3, 0.4, 0.5, 0.6]:\n",
    "                    data_dict = {\n",
    "                        \"System\": system,\n",
    "                        \"Sample Algorithm\": sample,\n",
    "                        \"Feature Selection\": ft,\n",
    "                        \"Algorithm\": algorithm,\n",
    "                        \"%Features\": features_number,\n",
    "                        \"#Features\": df_time_combined[(df_time_combined[\"%FeatureSelection\"] == features_number)][\"#Features\"],\n",
    "                        \"N=10%\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"%Configurations\"] == 0.1) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean(),\n",
    "                        \"N=20%\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"%Configurations\"] == 0.2) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean(),\n",
    "                        \"N=50%\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"%Configurations\"] == 0.5) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean(),\n",
    "                        \"N=80%\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"%Configurations\"] == 0.8) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean(),\n",
    "                        \"N=90%\": df_time_combined[(df_time_combined[\"FT\"] == ft) & (df_time_combined[\"%Configurations\"] == 0.9) & (df_time_combined[\"%FeatureSelection\"] == features_number)][\"ElapsedTime\"].mean(),\n",
    "                    }\n",
    "                data_list.append(data_dict)\n",
    "\n",
    "    \n",
    "    print(system)\n",
    "    display(pd.DataFrame(data_list))\n",
    "#     display(df_time_combined)\n",
    "    \n",
    "    df_time_summary = pd.DataFrame(data_list)\n",
    "#     display(df_time_summary)\n",
    "    df_time_per_sample = pd.DataFrame(data_list_sample)\n",
    "    df_time_all_systems = pd.concat([df_time_all_systems, pd.DataFrame(data_list_all)], ignore_index=True) \n",
    "    df_list.append(df_time_summary)\n",
    "    df_list.append(df_time_per_sample)\n",
    "    \n",
    "display(df_time_summary)    \n",
    "df_list.append(df_time_all_systems)    \n",
    "# display(df_time_all_systems)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10337a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# %pip install fpdf\n",
    "\n",
    "# from fpdf import FPDF\n",
    "\n",
    "\n",
    "\n",
    "df_Graph = pd.DataFrame({'without Feature Selection': df_time_all_systems['Total Elapsed Time'].values ,'with Feature Selection': df_time_all_systems['Total Elapsed Time with Feature Selection'].values}, df_time_all_systems['System'])\n",
    "display(df_Graph)\n",
    "\n",
    "# # df_Graph.export('results/table-simulation_time_all_system-Ntest.pdf')\n",
    "# plt.figure()  # Adjust the figure size as needed\n",
    "# plt.axis('off')  # Hide axes\n",
    "# plt.table(cellText=df_Graph.values, colLabels=df_Graph.columns, loc='center', cellLoc='center')\n",
    "# plt.savefig('results/table-simulation_time_all_system-Ntest.pdf', bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "\n",
    "df_Graph.plot(kind='bar')\n",
    "plt.xlabel('Systems')\n",
    "plt.ylabel('Elapsed Time (seconds)')\n",
    "# plt.show()\n",
    "plt.savefig('results/simulation_time_all_system-Ntest.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b301681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the graph to be plotted\n",
    "#systems = ['Dune', 'LLVM','x264','BerkeleyDBC','Irzip','Polly','Hipacc','7z']\n",
    "\n",
    "# system= system name ; detailed: True-detailed and False-resumed\n",
    "def choose_df(detailed=True, system=None):\n",
    "    df_return = df_list[len(df_list)-1]\n",
    "    series = pd.Series(systems)\n",
    "    \n",
    "    if system==None:\n",
    "        df_return = df_list[len(df_list)-1]\n",
    "    else:\n",
    "        index = series.index[series == system][0]\n",
    "        display(index)\n",
    "        if detailed:\n",
    "            df_return = df_list[index*2]\n",
    "        else:\n",
    "            df_return = df_list[index*2+1]\n",
    "\n",
    "    return df_return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type = False\n",
    "\n",
    "df_to_graph = choose_df(False,'Dune')\n",
    "mask = df_to_graph['Total Elapsed Time']!=0\n",
    "df_to_graph = df_to_graph[mask]\n",
    "display(df_to_graph)\n",
    "\n",
    "# display(df_to_graph['Total Elapsed Time'])\n",
    "# display(df_to_graph[df_to_graph['Feature Selection']==True]['Total Elapsed Time'])\n",
    "\n",
    "# if (type):\n",
    "# #     n10 = df_to_graph[df_to_graph['Feature Selection']==True][\"N=10%\"].values\n",
    "#     n10 = pd.DataFrame({'without Feature Selection': df_to_graph[df_to_graph['Feature Selection']==False][\"N=10%\"].values ,'with Feature Selection':df_to_graph[df_to_graph['Feature Selection']==True][\"N=10%\"].values}, samples_config)\n",
    "# #     df_to_graph = pd.DataFrame({'without Feature Selection': df_to_graph[False]['Total Elapsed Time'].values ,'with Feature Selection': df_to_graph[True]['Total Elapsed Time'].values}, df_to_graph['Sample Algorithm'])\n",
    "#     n10.plot(kind='bar')\n",
    "#     plt.xlabel('Sample Algorithm')\n",
    "#     plt.ylabel('Elapsed Time (seconds)')\n",
    "#     plt.show()\n",
    "# else:\n",
    "df_to_graph = pd.DataFrame({'without Feature Selection': df_to_graph[df_to_graph['Feature Selection']==False]['Total Elapsed Time'] ,'with Feature Selection': df_to_graph[df_to_graph['Feature Selection']==True]['Total Elapsed Time']}, df_to_graph['Sample Algorithm'])\n",
    "df_to_graph.plot(kind='bar')\n",
    "plt.xlabel('Sample Algorithm')\n",
    "plt.ylabel('Elapsed Time (seconds)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
